import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.metrics import confusion_matrix, classification_report
import itertools

# 1) Load & preprocess
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32")  / 255.0
y_train = y_train.ravel()
y_test  = y_test.ravel()
class_names = ['airplane','automobile','bird','cat','deer',
               'dog','frog','horse','ship','truck']

# 2) Build a small CNN (uses Input to avoid input_shape warning)
model = models.Sequential([
    layers.Input(shape=(32,32,3)),
    layers.Conv2D(32, 3, activation='relu', padding='same'),
    layers.MaxPooling2D(2),
    layers.Conv2D(64, 3, activation='relu', padding='same'),
    layers.MaxPooling2D(2),
    layers.Conv2D(128, 3, activation='relu', padding='same'),
    layers.GlobalAveragePooling2D(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# 3) Train
EPOCHS = 6
history = model.fit(x_train, y_train, epochs=EPOCHS,
                    batch_size=128, validation_split=0.1, verbose=2)

# 4) Plot training curves
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.xlabel('epoch'); plt.title('Loss'); plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.xlabel('epoch'); plt.title('Accuracy'); plt.legend()
plt.tight_layout()
plt.show()

# 5) Evaluate on test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test accuracy: {test_acc:.4f}")

# 6) Confusion matrix
y_pred = model.predict(x_test, verbose=0).argmax(axis=1)
cm = confusion_matrix(y_test, y_pred)

def plot_confusion_matrix(cm, classes, normalize=True):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
    plt.figure(figsize=(8,6))
    plt.imshow(cm, interpolation='nearest')
    plt.title('Confusion matrix' + (' (normalized)' if normalize else ''))
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45, ha='right')
    plt.yticks(tick_marks, classes)
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black", fontsize=7)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()

plot_confusion_matrix(cm, class_names, normalize=True)
print("\nClassification report (per-class precision/recall/f1):\n")
print(classification_report(y_test, y_pred, target_names=class_names, digits=3))

# 7) Show sample predictions (correct + wrong)
def show_examples(x, y_true, y_pred, correct=True, n=12):
    idxs = np.where(y_pred == y_true)[0] if correct else np.where(y_pred != y_true)[0]
    if len(idxs) == 0:
        print("No examples found.")
        return
    idxs = np.random.choice(idxs, size=min(n, len(idxs)), replace=False)
    plt.figure(figsize=(12,6))
    for i, idx in enumerate(idxs):
        plt.subplot(3, 4, i+1)
        plt.imshow(x[idx])
        plt.title(f"T:{class_names[y_true[idx]]}\nP:{class_names[y_pred[idx]]}")
        plt.axis('off')
    plt.suptitle('Correct' if correct else 'Misclassified')
    plt.show()

show_examples(x_test, y_test, y_pred, correct=True, n=12)
show_examples(x_test, y_test, y_pred, correct=False, n=12)

# 8) Save model (optional)
model.save('cifar_simple_model.h5')
print("Model saved to cifar_simple_model.h5")
